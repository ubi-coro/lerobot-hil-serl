We found that applying a random shift of 4 pixels is super effective, itâ€™s hard to find a simpler yet better augmentation. In my experience, another highly effective data augmentation technique is the  random overlay or random convolution, which enhances robustness to visual distractions  and sample efficiency for visual RL (SVEA/SODA-style augmentations https://arxiv.org/pdf/2011.13389 https://arxiv.org/pdf/2107.00644 ).
Honestly I find the RLPD, super complete and all necessary parameters are in the paper.

Another crucial trick for improving robustness and sample efficiency in visual RL, one that we didnâ€™t implement in our approach is consistency regularization. The idea is to match the Q-values of a state with the Q-values of its augmented counterpart.  During my PhD we wrote a paper using it two years ago, and by far, this was the most effective trick we tested (https://arxiv.org/pdf/2209.09203 this trick makes our methods sota).

An important detail is the initialization of the last layer of the policy. At the beginning of training for continuous control tasks, itâ€™s crucial to ensure a zero-mean action output. You can achieve this by initializing the last layer of the policy with very small values (we divided by 10 or 100 compared to the default initialization). While this doesnâ€™t make a difference in simulation, it significantly improves training stability for real robots. We took inspiration from this trick based on insights from this paper, even though their findings were tested only on on-policy methods (https://arxiv.org/pdf/2006.05990).

One major improvement is decoupling training from data collection significantly boosts sample efficiency by 5-10Ã— in my experiments. Honestly, I regret not implementing it during my phd, as it would have saved me countless hours of compute time and waiting for experiments to finish. ðŸ˜…