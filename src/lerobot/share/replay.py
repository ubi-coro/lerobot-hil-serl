

# features
# parallel actor
# parallel learner
#   for bc
#   for off-policy RL
# parallel recording

# datatsets and policies should be usable with lerobot_train

# single primitive wrapper

# everything works via environments
# robots and teleoperators can be dictionaries
# environments define their own make and processors









# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Records a dataset. Actions for the robot can be either generated by teleoperation or by a policy.

Example:

```shell
lerobot-record \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem58760431541 \
    --robot.cameras="{laptop: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}}" \
    --robot.id=black \
    --dataset.repo_id=<my_username>/<my_dataset_name> \
    --dataset.num_episodes=2 \
    --dataset.single_task="Grab the cube" \
    --display_data=true
    # <- Teleop optional if you want to teleoperate to record or in between episodes with a policy \
    # --teleop.type=so100_leader \
    # --teleop.port=/dev/tty.usbmodem58760431551 \
    # --teleop.id=blue \
    # <- Policy optional if you want to record with a policy \
    # --policy.path=${HF_USER}/my_policy \
```

Example recording with bimanual so100:
```shell
lerobot-record \
  --robot.type=bi_so100_follower \
  --robot.left_arm_port=/dev/tty.usbmodem5A460851411 \
  --robot.right_arm_port=/dev/tty.usbmodem5A460812391 \
  --robot.id=bimanual_follower \
  --robot.cameras='{
    left: {"type": "opencv", "index_or_path": 0, "width": 640, "height": 480, "fps": 30},
    top: {"type": "opencv", "index_or_path": 1, "width": 640, "height": 480, "fps": 30},
    right: {"type": "opencv", "index_or_path": 2, "width": 640, "height": 480, "fps": 30}
  }' \
  --teleop.type=bi_so100_leader \
  --teleop.left_arm_port=/dev/tty.usbmodem5A460828611 \
  --teleop.right_arm_port=/dev/tty.usbmodem5A460826981 \
  --teleop.id=bimanual_leader \
  --display_data=true \
  --dataset.repo_id=${HF_USER}/bimanual-so100-handover-cube \
  --dataset.num_episodes=25 \
  --dataset.single_task="Grab and handover the red cube to the other arm"
```
"""

import logging
import time
from dataclasses import asdict
from pprint import pformat
from typing import Any

import numpy as np
import torch

from lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig  # noqa: F401
from lerobot.cameras.realsense.configuration_realsense import RealSenseCameraConfig  # noqa: F401
from lerobot.configs import parser
from lerobot.datasets.image_writer import safe_stop_image_writer
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.datasets.video_utils import VideoEncodingManager
from lerobot.envs.robot_env import RobotEnv
from lerobot.policies.factory import make_policy, make_pre_post_processors
from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.processor import (
    PolicyAction,
    PolicyProcessorPipeline,
    RobotProcessorPipeline,
    create_transition,
    TransitionKey,
)
from lerobot.processor.rename_processor import rename_stats
from lerobot.rl.gym_manipulator import step_env_and_process_transition
from lerobot.robots import (  # noqa: F401
    Robot,
    RobotConfig,
    bi_so100_follower,
    hope_jr,
    koch_follower,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
from lerobot.share.configs import RecordConfig
from lerobot.teleoperators import (  # noqa: F401
    Teleoperator,
    TeleoperatorConfig,
    bi_so100_leader,
    homunculus,
    koch_leader,
    make_teleoperator_from_config,
    so100_leader,
    so101_leader, TeleopEvents,
)
from lerobot.utils.constants import ACTION, REWARD, DONE, OBS_STR
from lerobot.utils.control_utils import (
    predict_action,
    sanity_check_dataset_name,
    sanity_check_dataset_robot_compatibility,
)
from lerobot.utils.robot_utils import busy_wait
from lerobot.utils.transition import Transition
from lerobot.utils.utils import (
    get_safe_torch_device,
    init_logging,
    log_say,
)
from lerobot.utils.visualization_utils import init_rerun, log_rerun_data
from lerobot.share.utils import get_pipeline_dataset_features


""" --------------- record_loop() data flow --------------------------
       [ Robot ]
           V
     [ robot.get_observation() ] ---> raw_obs
           V
     [ robot_observation_processor ] ---> processed_obs
           V
     .-----( ACTION LOGIC )------------------.
     V                                       V
     [ From Teleoperator ]                   [ From Policy ]
     |                                       |
     |  [teleop.get_action] -> raw_action    |   [predict_action]
     |          |                            |          |
     |          V                            |          V
     | [teleop_action_processor]             |          |
     |          |                            |          |
     '---> processed_teleop_action           '---> processed_policy_action
     |                                       |
     '-------------------------.-------------'
                               V
                  [ robot_action_processor ] --> robot_action_to_send
                               V
                    [ robot.send_action() ] -- (Robot Executes)
                               V
                    ( Save to Dataset )
                               V
                  ( Rerun Log / Loop Wait )
"""


@safe_stop_image_writer
def record_loop(
    env: RobotEnv,
    fps: int,
    action_dim: int,
    action_processor: RobotProcessorPipeline[Transition, Transition],
    env_processor: RobotProcessorPipeline[Transition, Transition],
    dataset: LeRobotDataset | None = None,
    policy: PreTrainedPolicy | None = None,
    preprocessor: PolicyProcessorPipeline[dict[str, Any], dict[str, Any]] | None = None,
    postprocessor: PolicyProcessorPipeline[PolicyAction, PolicyAction] | None = None,
    control_time_s: float | None = None,
    single_task: str | None = None,
    robot_type: str| None = None,
    display_data: bool = False,
    device: str = "cuda",
    use_amp: bool = False,
    interactive: bool = False
):
    if control_time_s is None:
        control_time_s = float("inf")

    if dataset is not None and dataset.fps != fps:
        raise ValueError(f"The dataset fps should be equal to requested fps ({dataset.fps} != {fps}).")

    has_policy = policy is not None and preprocessor is not None and postprocessor is not None
    teleoperate = not has_policy
    assert not interactive or has_policy, "Interactive recording requires a policy."

    # Reset policy and processor if they are provided
    if policy is not None and preprocessor is not None and postprocessor is not None:
        policy.reset()
        preprocessor.reset()
        postprocessor.reset()

    obs, info = env.reset()
    env_processor.reset()
    action_processor.reset()

    # Process initial observation
    transition = create_transition(observation=obs, info=info)
    transition = env_processor(data=transition)  # outputs valid transition

    intervention_occurred = False
    info: dict = transition[TransitionKey.INFO]
    episode_step = 0
    episode_start_time = time.perf_counter()
    print("Teleoperate" if teleoperate else "Record")
    while (time.perf_counter() - episode_start_time) < control_time_s:
        start_loop_t = time.perf_counter()

        # (1) Keep the PRE-STEP transition (this holds o_t) and reset info dict
        prev_transition = transition
        info = {}

        # (2) Handle intervention control flow
        if teleoperate:
            # Permanently set the intervention flag to stay in control
            info[TeleopEvents.IS_INTERVENTION] = True

        # (3) Decide action a_t
        if has_policy:
            policy_observation = {
                k: v for k, v in prev_transition[TransitionKey.OBSERVATION].items() if k in policy.config.input_features
            }
            # noinspection PyTypeChecker
            action = predict_action(
                observation=policy_observation,
                policy=policy,
                device=get_safe_torch_device(device),
                preprocessor=preprocessor,
                postprocessor=postprocessor,
                use_amp=use_amp,
                task=single_task,
                robot_type=robot_type
            )
        else:
            # Dummy action, expected to be overwritten by teleop action
            action = torch.tensor([0.0] * action_dim, dtype=torch.float32)

        # (4) Step the env -> returns NEW transition with o_{t+1}, r_{t+1}, done
        transition = step_env_and_process_transition(
            env=env,
            action=action,
            env_processor=env_processor,
            action_processor=action_processor,
            info=info
        )

        action = transition[TransitionKey.ACTION]
        reward = transition[TransitionKey.REWARD]
        terminated = transition.get(TransitionKey.DONE, False)
        truncated = transition.get(TransitionKey.TRUNCATED, False)
        info = transition.get(TransitionKey.INFO, {})

        # (5) Store transition. When interactive, only store frames on interventions
        if dataset is not None and (not interactive or info.get(TeleopEvents.IS_INTERVENTION, False)):

            # observations are batched and may contain other keys
            dataset_observation = {
                k: v.squeeze().cpu()
                for k, v in prev_transition[TransitionKey.OBSERVATION].items()
                if k in dataset.features
            }

            # store frame
            frame = {
                **dataset_observation,
                ACTION: action.squeeze().cpu(),
                REWARD: np.array([reward], dtype=np.float32),
                DONE: np.array([terminated or truncated], dtype=bool),
                "task": single_task
            }
            dataset.add_frame(frame)

            if display_data:
                rerun_obs = {k: v.numpy() for k, v in dataset_observation.items()}
                log_rerun_data(observation=rerun_obs, action=action)

        episode_step += 1

        # (6) Handle done
        if terminated or truncated:
            episode_time = time.perf_counter() - episode_start_time
            logging.info(
                f"Episode ended after {episode_step} steps in {episode_time:.1f}s with reward {transition[TransitionKey.REWARD]}"
            )
            return info

        # Terminate on intervention end to correctly store episode bounds
        if interactive:
            intervention_occurred = intervention_occurred | info.get(TeleopEvents.IS_INTERVENTION, False)
            if intervention_occurred and not info.get(TeleopEvents.IS_INTERVENTION, False):
                info[TeleopEvents.INTERVENTION_COMPLETED] = True
                episode_time = time.perf_counter() - episode_start_time
                logging.info(
                    f"Intervention ended after {episode_step} steps in {episode_time:.1f}s with reward {transition[TransitionKey.REWARD]}"
                )
                return info

        if info.get(TeleopEvents.RERECORD_EPISODE, False) or info.get(TeleopEvents.TERMINATE_EPISODE, False):
            return info

        # (7) Handle frequency
        dt_load = time.perf_counter() - start_loop_t
        busy_wait(1 / fps - dt_load)
        dt_loop = time.perf_counter() - start_loop_t
        logging.info(
            f"dt_load: {dt_loop * 1000:5.2f}ms ({1 / dt_loop:3.1f}hz), "
            f"dt_load: {dt_load * 1000:5.2f}ms ({1 / dt_load:3.1f}hz)"
        )

    else:
        return info

@parser.wrap()
def replay(cfg: RecordConfig) -> LeRobotDataset:
    replay_episode = 0

    env, env_processor, action_processor = cfg.env.make(device="cpu" if cfg.policy is None else cfg.policy.device)

    dataset = LeRobotDataset(
        cfg.dataset.repo_id,
        root=cfg.dataset.root,
        batch_encoding_size=cfg.dataset.video_encoding_batch_size,
    )
    episode_frames = dataset.hf_dataset.filter(lambda x: x["episode_index"] == replay_episode)
    actions = episode_frames.select_columns(ACTION)

    env.reset()
    env_processor.reset()
    action_processor.reset()

    for action_data in actions:
        start_time = time.perf_counter()
        step_env_and_process_transition(
            env=env,
            action=action_data[ACTION],
            env_processor=env_processor,
            action_processor=action_processor,
            info={}
        )
        busy_wait(1 / cfg.env.fps - (time.perf_counter() - start_time))

if __name__ == "__main__":
    import experiments
    replay()
